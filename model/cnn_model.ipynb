{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as Fn\n",
    "from torch import optim\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNArchitecture(nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super(CNNArchitecture, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.Pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 , 256)  # Adjusted for the output size after pooling\n",
    "        # self.fc2 = nn.Linear(1024, 512)\n",
    "        # self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        # self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(128, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convolutional layers with ReLU activations and pooling\n",
    "        x = Fn.relu(self.conv1(x))\n",
    "        x = self.Pool(x)\n",
    "        x = Fn.relu(self.conv2(x))\n",
    "        x = self.Pool(x)\n",
    "        x = Fn.relu(self.conv3(x))\n",
    "        x = self.Pool(x)\n",
    "        x = Fn.relu(self.conv4(x))\n",
    "        x = self.Pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output to (batch_size, num_features)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Fully connected layers with ReLU activations\n",
    "        x = Fn.relu(self.fc1(x))\n",
    "        x = Fn.relu(self.fc2(x))\n",
    "        x = Fn.relu(self.fc3(x))\n",
    "        x = Fn.relu(self.fc4(x))\n",
    "        x = Fn.relu(self.fc5(x))\n",
    "\n",
    "        # Final output layer (no ReLU)\n",
    "        x = self.fc6(x)\n",
    "        # print(x.shape)\n",
    "        # out = torch.argmax(x, dim=1).float()\n",
    "        # print(out)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed collecting all the data points X: (11965, 128, 173) and Y: (11965,)\n",
      "10768 1197\n",
      "Completed collecting all the data points X: (11965, 128, 173) and Y: (11965,)\n",
      "Epoch 1/10, Loss: 29.1815, Accuracy: 90.0167%\n",
      "Epoch 2/10, Loss: 0.0000, Accuracy: 100.0000%\n",
      "Stopping the Training as Accuracy has reached to Maximum\n",
      "Model saved to Model Path\n"
     ]
    }
   ],
   "source": [
    "class CNN1LayerArch(nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super(CNN1LayerArch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.Pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(45056 , 128)  # Adjusted for the output size after pooling\n",
    "        # self.fc4 = nn.Linear(256, 128)\n",
    "        # self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(128, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convolutional layers with ReLU activations and pooling\n",
    "        x = Fn.relu(self.conv1(x))\n",
    "        x = self.Pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output to (batch_size, num_features)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Fully connected layers with ReLU activations\n",
    "        x = Fn.relu(self.fc1(x))\n",
    "        # Final output layer (no ReLU)\n",
    "        x = self.fc6(x)\n",
    "        # print(x.shape)\n",
    "        # out = torch.argmax(x, dim=1).float()\n",
    "        # print(out)\n",
    "\n",
    "        return x\n",
    "\n",
    "# model = CNN1LayerArch(6)\n",
    "# img = torch.randn(64, 1, 128, 173)\n",
    "# tensor = torch.tensor(img)\n",
    "# print(model(img).shape)\n",
    "\n",
    "import data_loader #import getDatapoints\n",
    "\n",
    "trainDataLoader, testDataLoader = data_loader.getDatapoints()\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "epochs = 10\n",
    "model = CNN1LayerArch(classes=6)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True \n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0 \n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_id, (trainX, trainY) in enumerate(trainDataLoader):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        # Add channel dimension (for Conv2d)\n",
    "        trainX = trainX.unsqueeze(1)  # Adds a channel dimension at position 1\n",
    "        trainX = trainX.to(device)\n",
    "        trainY = trainY.long()\n",
    "        trainY = trainY.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(trainX)\n",
    "        \n",
    "        # print(\"Predictions dtype:\", pred.dtype)  # This should be torch.float32\n",
    "        # trainY.dtype = torch.long()\n",
    "        # print(\"Targets dtype:\", trainY.dtype) \n",
    "        # Compute the loss\n",
    "        lossval = lossFn(pred, trainY)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Check if the loss tensor requires gradients\n",
    "        # print(\"Check: \", lossval.requires_grad)  # This should print True now\n",
    "\n",
    "        # Backward pass\n",
    "        lossval.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += lossval.item()  # Add batch loss to epoch loss\n",
    "        \n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time  # Time taken for the batch\n",
    "        # print(f\"Batch {batch_id + 1}, Time per batch: {batch_time:.4f} seconds\")\n",
    "\n",
    "        # break  # For debugging, you can remove this to train on the full dataset\n",
    "        with torch.no_grad():  # No gradient computation for accuracy\n",
    "            predictions = torch.argmax(pred, dim=1)  # Get predicted class labels\n",
    "            correct_predictions += (predictions == trainY).sum().item()  # Count correct predictions\n",
    "            total_samples += trainY.size(0)  # Update total number of samples\n",
    "    average_loss = epoch_loss / len(trainDataLoader)  # Average loss\n",
    "    accuracy = correct_predictions / total_samples * 100  # Accuracy as percentage\n",
    "\n",
    "    # Display metrics for the epoch\n",
    "    print(f\"Epoch {each_epoch + 1}/{epochs}, Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "    if(accuracy >= 99):\n",
    "        print(f\"Stopping the Training as Accuracy has reached to Maximum\")\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f = \"/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/CNN1Layermodel.pth\")\n",
    "print(f\"Model saved to Model Path\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed collecting all the data points X: (23932, 128, 173) and Y: (23932,)\n",
      "21538 2394\n",
      "Completed collecting all the data points X: (23932, 128, 173) and Y: (23932,)\n"
     ]
    }
   ],
   "source": [
    "import data_loader #import getDatapoints\n",
    "\n",
    "trainDataLoader, testDataLoader = data_loader.getDatapoints()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1053, Accuracy: 95.0367%\n",
      "Epoch 2/10, Loss: 0.4029, Accuracy: 91.8609%\n",
      "Epoch 3/10, Loss: 0.0000, Accuracy: 100.0000%\n",
      "Stopping the Training as Accuracy has reached to Maximum\n",
      "Model saved to Model Path\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "epochs = 10\n",
    "model = CNNArchitecture(classes=6)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True \n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0 \n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_id, (trainX, trainY) in enumerate(trainDataLoader):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        # Add channel dimension (for Conv2d)\n",
    "        trainX = trainX.unsqueeze(1)  # Adds a channel dimension at position 1\n",
    "        trainX = trainX.to(device)\n",
    "        trainY = trainY.long()\n",
    "        trainY = trainY.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(trainX)\n",
    "        \n",
    "        # print(\"Predictions dtype:\", pred.dtype)  # This should be torch.float32\n",
    "        # trainY.dtype = torch.long()\n",
    "        # print(\"Targets dtype:\", trainY.dtype) \n",
    "        # Compute the loss\n",
    "        lossval = lossFn(pred, trainY)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Check if the loss tensor requires gradients\n",
    "        # print(\"Check: \", lossval.requires_grad)  # This should print True now\n",
    "\n",
    "        # Backward pass\n",
    "        lossval.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += lossval.item()  # Add batch loss to epoch loss\n",
    "        \n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time  # Time taken for the batch\n",
    "        # print(f\"Batch {batch_id + 1}, Time per batch: {batch_time:.4f} seconds\")\n",
    "\n",
    "        # break  # For debugging, you can remove this to train on the full dataset\n",
    "        with torch.no_grad():  # No gradient computation for accuracy\n",
    "            predictions = torch.argmax(pred, dim=1)  # Get predicted class labels\n",
    "            correct_predictions += (predictions == trainY).sum().item()  # Count correct predictions\n",
    "            total_samples += trainY.size(0)  # Update total number of samples\n",
    "    average_loss = epoch_loss / len(trainDataLoader)  # Average loss\n",
    "    accuracy = correct_predictions / total_samples * 100  # Accuracy as percentage\n",
    "\n",
    "    # Display metrics for the epoch\n",
    "    print(f\"Epoch {each_epoch + 1}/{epochs}, Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "    if(accuracy >= 99):\n",
    "        print(f\"Stopping the Training as Accuracy has reached to Maximum\")\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f = \"/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/CNNModel.pth\")\n",
    "print(f\"Model saved to Model Path\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training without Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;66;03m#import getDatapoints\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m trainDataLoader, testDataLoader \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDatapoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/ser/Speech-Emotion-Recognition/model/data_loader.py:53\u001b[0m, in \u001b[0;36mgetDatapoints\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m audioPath \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][index]\n\u001b[1;32m     52\u001b[0m audioClass \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m][index]\n\u001b[0;32m---> 53\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[43mgetMelVector\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudioPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetDuration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m X\u001b[38;5;241m.\u001b[39mappend(vector)\n\u001b[1;32m     55\u001b[0m Y\u001b[38;5;241m.\u001b[39mappend(audioClass)\n",
      "File \u001b[0;32m~/Desktop/ser/Speech-Emotion-Recognition/model/data_loader.py:24\u001b[0m, in \u001b[0;36mgetMelVector\u001b[0;34m(audio_path, targetDuration)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetMelVector\u001b[39m(audio_path, targetDuration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5.0\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     audio_series, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22050\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     target_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(targetDuration \u001b[38;5;241m*\u001b[39m sampling_rate) \u001b[38;5;66;03m# Total Number of Samples from the Audio \u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(audio_series) \u001b[38;5;241m<\u001b[39m target_samples:\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt/lib/python3.11/site-packages/librosa/core/audio.py:193\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    190\u001b[0m     y \u001b[38;5;241m=\u001b[39m to_mono(y)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr_native\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     sr \u001b[38;5;241m=\u001b[39m sr_native\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt/lib/python3.11/site-packages/librosa/core/audio.py:669\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, axis, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\n\u001b[1;32m    664\u001b[0m         samplerate\u001b[38;5;241m.\u001b[39mresample, axis\u001b[38;5;241m=\u001b[39maxis, arr\u001b[38;5;241m=\u001b[39my, ratio\u001b[38;5;241m=\u001b[39mratio, converter_type\u001b[38;5;241m=\u001b[39mres_type\n\u001b[1;32m    665\u001b[0m     )\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoxr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;66;03m# Use numpy to vectorize the resampler along the target axis\u001b[39;00m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;66;03m# This is because soxr does not support ndim>2 generally.\u001b[39;00m\n\u001b[0;32m--> 669\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_sr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_sr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m resampy\u001b[38;5;241m.\u001b[39mresample(y, orig_sr, target_sr, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mres_type, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt/lib/python3.11/site-packages/numpy/lib/shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43mfunc1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minarr_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind0\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[1;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt/lib/python3.11/site-packages/soxr/__init__.py:206\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(x, in_rate, out_rate, quality)\u001b[0m\n\u001b[1;32m    203\u001b[0m q \u001b[38;5;241m=\u001b[39m _quality_to_enum(quality)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 206\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mdivide_proc\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqueeze(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import data_loader #import getDatapoints\n",
    "\n",
    "trainDataLoader, testDataLoader = data_loader.getDatapoints()\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "epochs = 10\n",
    "model = CNNArchitecture(classes=6)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True \n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0 \n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_id, (trainX, trainY) in enumerate(trainDataLoader):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        # Add channel dimension (for Conv2d)\n",
    "        trainX = trainX.unsqueeze(1)  # Adds a channel dimension at position 1\n",
    "        trainX = trainX.to(device)\n",
    "        trainY = trainY.long()\n",
    "        trainY = trainY.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(trainX)\n",
    "        \n",
    "        # print(\"Predictions dtype:\", pred.dtype)  # This should be torch.float32\n",
    "        # trainY.dtype = torch.long()\n",
    "        # print(\"Targets dtype:\", trainY.dtype) \n",
    "        # Compute the loss\n",
    "        lossval = lossFn(pred, trainY)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Check if the loss tensor requires gradients\n",
    "        # print(\"Check: \", lossval.requires_grad)  # This should print True now\n",
    "\n",
    "        # Backward pass\n",
    "        lossval.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += lossval.item()  # Add batch loss to epoch loss\n",
    "        \n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time  # Time taken for the batch\n",
    "        # print(f\"Batch {batch_id + 1}, Time per batch: {batch_time:.4f} seconds\")\n",
    "\n",
    "        # break  # For debugging, you can remove this to train on the full dataset\n",
    "        with torch.no_grad():  # No gradient computation for accuracy\n",
    "            predictions = torch.argmax(pred, dim=1)  # Get predicted class labels\n",
    "            correct_predictions += (predictions == trainY).sum().item()  # Count correct predictions\n",
    "            total_samples += trainY.size(0)  # Update total number of samples\n",
    "    average_loss = epoch_loss / len(trainDataLoader)  # Average loss\n",
    "    accuracy = correct_predictions / total_samples * 100  # Accuracy as percentage\n",
    "\n",
    "    # Display metrics for the epoch\n",
    "    print(f\"Epoch {each_epoch + 1}/{epochs}, Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "    if(accuracy >= 99):\n",
    "        print(f\"Stopping the Training as Accuracy has reached to Maximum\")\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f = \"/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/CNNModel_NoNorm.pth\")\n",
    "print(f\"Model saved to Model Path\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y8/q2s37ndx6tg3lpzp3vp8xk_r0000gn/T/ipykernel_5314/50456109.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "class CNNArchitecture2(nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super(CNNArchitecture2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        # self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        # self.conv4 = nn.Conv2d(in_channels=32, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.Pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(5632 , 256)  # Adjusted for the output size after pooling\n",
    "        # self.fc2 = nn.Linear(1024, 512)\n",
    "        # self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        # self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(128, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convolutional layers with ReLU activations and pooling\n",
    "        x = Fn.relu(self.conv1(x))\n",
    "        x = self.Pool(x)\n",
    "        x = Fn.relu(self.conv2(x))\n",
    "        x = self.Pool(x)\n",
    "        # x = Fn.relu(self.conv3(x))\n",
    "        # x = self.Pool(x)\n",
    "        # x = Fn.relu(self.conv4(x))\n",
    "        # x = self.Pool(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output to (batch_size, num_features)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Fully connected layers with ReLU activations\n",
    "        x = Fn.relu(self.fc1(x))\n",
    "        x = Fn.dropout(x, 0.4)\n",
    "        # x = Fn.relu(self.fc2(x))\n",
    "        # x = Fn.relu(self.fc3(x))\n",
    "        x = Fn.relu(self.fc4(x))\n",
    "        x = Fn.dropout(x, 0.4)\n",
    "        # x = Fn.relu(self.fc5(x))\n",
    "\n",
    "        # Final output layer (no ReLU)\n",
    "        x = self.fc6(x)\n",
    "        # print(x.shape)\n",
    "        # out = torch.argmax(x, dim=1).float()\n",
    "        # print(out)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CNNArchitecture2(6)\n",
    "img = torch.randn(64, 1, 128, 173)\n",
    "tensor = torch.tensor(img)\n",
    "print(model(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2512, Accuracy: 92.0698%\n",
      "Epoch 2/10, Loss: 0.0005, Accuracy: 99.9861%\n",
      "Stopping the Training as Accuracy has reached to Maximum\n",
      "Model saved to Model Path\n"
     ]
    }
   ],
   "source": [
    "# import data_loader #import getDatapoints\n",
    "\n",
    "# trainDataLoader, testDataLoader = data_loader.getDatapoints()\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "epochs = 10\n",
    "model = CNNArchitecture2(classes=6)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True \n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0 \n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_id, (trainX, trainY) in enumerate(trainDataLoader):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        # Add channel dimension (for Conv2d)\n",
    "        trainX = trainX.unsqueeze(1)  # Adds a channel dimension at position 1\n",
    "        trainX = trainX.to(device)\n",
    "        trainY = trainY.long()\n",
    "        trainY = trainY.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(trainX)\n",
    "        \n",
    "        # print(\"Predictions dtype:\", pred.dtype)  # This should be torch.float32\n",
    "        # trainY.dtype = torch.long()\n",
    "        # print(\"Targets dtype:\", trainY.dtype) \n",
    "        # Compute the loss\n",
    "        lossval = lossFn(pred, trainY)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Check if the loss tensor requires gradients\n",
    "        # print(\"Check: \", lossval.requires_grad)  # This should print True now\n",
    "\n",
    "        # Backward pass\n",
    "        lossval.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += lossval.item()  # Add batch loss to epoch loss\n",
    "        \n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time  # Time taken for the batch\n",
    "        # print(f\"Batch {batch_id + 1}, Time per batch: {batch_time:.4f} seconds\")\n",
    "\n",
    "        # break  # For debugging, you can remove this to train on the full dataset\n",
    "        with torch.no_grad():  # No gradient computation for accuracy\n",
    "            predictions = torch.argmax(pred, dim=1)  # Get predicted class labels\n",
    "            correct_predictions += (predictions == trainY).sum().item()  # Count correct predictions\n",
    "            total_samples += trainY.size(0)  # Update total number of samples\n",
    "    average_loss = epoch_loss / len(trainDataLoader)  # Average loss\n",
    "    accuracy = correct_predictions / total_samples * 100  # Accuracy as percentage\n",
    "\n",
    "    # Display metrics for the epoch\n",
    "    print(f\"Epoch {each_epoch + 1}/{epochs}, Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "    if(accuracy >= 99):\n",
    "        print(f\"Stopping the Training as Accuracy has reached to Maximum\")\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f = \"/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/CNNModelBN_test.pth\")\n",
    "print(f\"Model saved to Model Path\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is  100.0\n"
     ]
    }
   ],
   "source": [
    "test_model = CNN1LayerArch(classes=6)\n",
    "# Load the saved model weights\n",
    "# test_model.load_state_dict(torch.load('/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/CNNModel_NoNorm.pth'))\n",
    "test_model.load_state_dict(torch.load('/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/CNN1Layermodel.pth'))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "# 5. Set the model to evaluation mode (important for inference)\n",
    "\n",
    "# the model to evaluation mode (important for inference)\n",
    "test_model.eval()\n",
    "\n",
    "def testAccuracy(model, loader):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "\n",
    "            x = x.unsqueeze(1)  # Adds a channel dimension at position 1\n",
    "            y = y.long()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            scores = test_model(x)\n",
    "\n",
    "            predictions = torch.argmax(scores, dim=1)\n",
    "            # print(predictions, \"-----\", y)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    accuracy = num_correct / num_samples * 100  # Accuracy as percentage\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "testAccuracy = testAccuracy(test_model, testDataLoader)\n",
    "print(f\"Test Accuracy is  {testAccuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  8.3698,  19.1293,   7.8895,  -3.0344, -13.2437,  -8.3052]])\n",
      "tensor([1]) ----- tensor([1])\n"
     ]
    }
   ],
   "source": [
    "from data_loader import getMelVector\n",
    "test_audio = getMelVector(\"/Users/ishananand/Desktop/ser/testAudios/angry.wav\", 4)\n",
    "X, Y = [], []\n",
    "X.append(test_audio)\n",
    "Y.append(1)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y = Y.astype(np.int64)\n",
    "# X = (X - (-1 * 0.2380136)) / 0.07312169  # This normalization will use the same mean and std for all images\n",
    "X_tensor = torch.tensor(X)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.long)  # Use torch.float32 for regression, torch.long for classification\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "customLoader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "# customLoader.dataset\n",
    "with torch.no_grad():\n",
    "    for x, y in customLoader:\n",
    "\n",
    "        x = x.unsqueeze(1)  # Adds a channel dimension at position 1\n",
    "        y = y.long()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        scores = test_model(x)\n",
    "        print(scores)\n",
    "\n",
    "        predictions = torch.argmax(scores, dim=1)\n",
    "        print(predictions, \"-----\", y)\n",
    "\n",
    "    test_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ishananand/Desktop/ser/testAudios/happy.wav\n",
      "/Users/ishananand/Desktop/ser/testAudios/sad.wav\n",
      "/Users/ishananand/Desktop/ser/testAudios/sad_G.wav\n",
      "/Users/ishananand/Desktop/ser/testAudios/fear.wav\n",
      "/Users/ishananand/Desktop/ser/testAudios/angry.wav\n",
      "/Users/ishananand/Desktop/ser/testAudios/disgust.wav\n",
      "/Users/ishananand/Desktop/ser/testAudios/happy1.wav\n",
      "/Users/ishananand/Desktop/ser/testAudios/neutral1.wav\n",
      "torch.Size([1, 1, 128, 173])\n",
      "tensor([[-14.8378, -24.9900, -13.0976, -53.7655, -50.0690, -34.1688]])\n",
      "tensor([2]) ----- 0\n",
      "torch.Size([1, 1, 128, 173])\n",
      "tensor([[-24.1275, -25.6195, -26.1210, -55.8879, -39.0649, -19.3158]])\n",
      "tensor([5]) ----- 3\n",
      "torch.Size([1, 1, 128, 173])\n",
      "tensor([[-29.3504, -29.0441, -26.7591, -25.7494, -65.4517, -51.1445]])\n",
      "tensor([3]) ----- 3\n",
      "torch.Size([1, 1, 128, 173])\n",
      "tensor([[-16.9629, -26.3708, -13.8111, -54.4327, -42.9009, -23.0323]])\n",
      "tensor([2]) ----- 2\n",
      "torch.Size([1, 1, 128, 173])\n",
      "tensor([[ -5.1884, -32.2631,   0.3413, -57.4533, -46.4341, -31.9876]])\n",
      "tensor([2]) ----- 1\n",
      "torch.Size([1, 1, 128, 173])\n",
      "tensor([[-24.7689, -24.5722, -16.4198, -41.0242, -60.5516, -31.4211]])\n",
      "tensor([2]) ----- 4\n",
      "torch.Size([1, 1, 128, 173])\n",
      "tensor([[-18.0287, -31.8141, -32.8352, -61.4052, -29.8085, -32.2048]])\n",
      "tensor([0]) ----- 0\n",
      "torch.Size([1, 1, 128, 173])\n",
      "tensor([[-25.0872, -33.0522, -30.6752, -49.0830, -43.8730, -39.6922]])\n",
      "tensor([0]) ----- 5\n"
     ]
    }
   ],
   "source": [
    "from data_loader import getMelVector\n",
    "import os\n",
    "# test_model.load_state_dict(torch.load('/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/test_model.pth'))\n",
    "# test_model.load_state_dict(torch.load('/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/CNNModel.pth'))\n",
    "# test_model = CNNArchitecture2(classes=6)\n",
    "# Load the saved model weights\n",
    "# test_model.load_state_dict(torch.load('/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/CNNModel_NoNorm.pth'))\n",
    "# test_model.load_state_dict(torch.load('/Users/ishananand/Desktop/ser/Speech-Emotion-Recognition/model_weight/CNNModelBN_test.pth'))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "# 5. Set the model to evaluation mode (important for inference)\n",
    "\n",
    "# the model to evaluation mode (important for inference)\n",
    "test_model.eval()\n",
    "\n",
    "allAudios = os.listdir(\"/Users/ishananand/Desktop/ser/testAudios\")\n",
    "rootpath = \"/Users/ishananand/Desktop/ser/testAudios\"\n",
    "emotion_class = {\n",
    "    0: \"happy\",\n",
    "    1: \"angry\",\n",
    "    2: \"fear\",\n",
    "    3: \"sad\",\n",
    "    4: \"disgust\",\n",
    "    5: \"neutral\"\n",
    "}\n",
    "X, Y = [], []\n",
    "for each_audio in allAudios:\n",
    "    print(rootpath + \"/\" +each_audio)\n",
    "\n",
    "    test_audio = getMelVector(rootpath + \"/\" +each_audio, 4)\n",
    "    # print(test_audio)\n",
    "    \n",
    "    X.append(test_audio)\n",
    "    if(\"angry\" in each_audio):\n",
    "        Y.append(1)\n",
    "    elif(\"disgust\" in each_audio):\n",
    "        Y.append(4)\n",
    "    elif(\"happy\" in each_audio):\n",
    "        Y.append(0)\n",
    "    elif(\"sad\" in each_audio):\n",
    "        Y.append(3)\n",
    "    elif(\"neutral\" in each_audio):\n",
    "        Y.append(5)\n",
    "    elif(\"fear\" in each_audio):\n",
    "        Y.append(2)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y = Y.astype(np.int64)\n",
    "# X = (X - (-1 * 0.2380136)) / 0.07312169  # This normalization will use the same mean and std for all images\n",
    "X_tensor = torch.tensor(X)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.long)  # Use torch.float32 for regression, torch.long for classification\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "customLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "# customLoader.dataset\n",
    "with torch.no_grad():\n",
    "    for x, y in customLoader:\n",
    "\n",
    "        x = x.unsqueeze(1)  # Adds a channel dimension at position 1\n",
    "        y = y.long()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        print(x.shape)\n",
    "        scores = test_model(x)\n",
    "        print(scores)\n",
    "        y = int(y.to(torch.int32))\n",
    "        predictions = torch.argmax(scores, dim=1)\n",
    "        tensor_int = int(predictions.to(torch.int32))\n",
    "        # print(customLoader.)\n",
    "        print(predictions, \"-----\", y)\n",
    "        # print(f\" The true Value is {emotion_class[y]} and predicted class is {emotion_class[tensor_int]}\")\n",
    "        # print(scores)\n",
    "\n",
    "        \n",
    "        # print(predictions)\n",
    "\n",
    "    test_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
